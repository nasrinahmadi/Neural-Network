{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " employee churn prediction using Multi-Layer Perceptron"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load dataset\n",
    "Let’s first load the required HR dataset using pandas’ read CSV function. You can download data from https://www.kaggle.com/datasets/liujiaqi/hr-comma-sepcsv "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Load data\n",
    "data=pd.read_csv('HR_comma_sep.csv')\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing: Label Encoding\n",
    "Lots of machine learning algorithms require numerical input data, so you need to represent categorical columns in a numerical column. In order to encode this data, you could map each value to a number. e.g. Salary column’s value can be represented as low:0, medium:1, and high:2. This process is known as label encoding. In sklearn, we can do this using LabelEncoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import LabelEncoder\n",
    "from sklearn import preprocessing\n",
    "\n",
    "# Creating labelEncoder\n",
    "le = preprocessing.LabelEncoder()\n",
    "\n",
    "# Converting string labels into numbers.\n",
    "data['salary']=le.fit_transform(data['salary'])\n",
    "data['Departments ']=le.fit_transform(data['Departments '])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we imported the preprocessing module and created the Label Encoder object. Using this LabelEncoder object you fit and transform the “salary” and “Departments “ column into the numeric column."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the dataset\n",
    "In order to assess the model performance, we need to divide the dataset into a training set and a test set. Let’s split dataset by using function train_test_split(). you need to pass basically 3 parameters features, target, and test_set size. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spliting data into Feature and\n",
    "X=data[['satisfaction_level', 'last_evaluation', 'number_project', 'average_montly_hours', 'time_spend_company', 'Work_accident', 'promotion_last_5years', 'Departments ', 'salary']]\n",
    "y=data['left']\n",
    "\n",
    "# Import train_test_split function\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split dataset into training set and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)  # 70% training and 30% test"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build Classification Model\n",
    "Let’s build an employee churn prediction model. Here, our objective is to predict churn using MLPClassifier.\n",
    "\n",
    "First, import the MLPClassifier module and create MLP Classifier object using MLPClassifier() function. Then, fit your model on the train set using fit() and perform prediction on the test set using predict()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import MLPClassifer \n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# Create model object\n",
    "clf = MLPClassifier(hidden_layer_sizes=(6,5),\n",
    "                    random_state=5,\n",
    "                    verbose=True,\n",
    "                    learning_rate_init=0.01)\n",
    "\n",
    "# Fit data onto the model\n",
    "clf.fit(X_train,y_train)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameters:\n",
    "\n",
    "hidden_layer_sizes: it is a tuple where each element represents one layer and its value represents the number of neurons on each hidden layer.\n",
    "learning_rate_init: It used to controls the step-size in updating the weights. \n",
    "activation: Activation function for the hidden layer. Examples, identity, logistic, tanh, and relu. by default, relu is used as an activation function.\n",
    "random_state: It defines the random number for weights and bias initialization. \n",
    "verbose: It used to print progress messages to standard output."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make Prediction and Evaluate the Model\n",
    "In this section, we will make predictions on the test dataset and assess model accuracy based on available actual labels of the test dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make prediction on test dataset\n",
    "ypredict=clf.predict(X_test)\n",
    "\n",
    "# Import accuracy score \n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Calcuate accuracy\n",
    "accuracy_score(y_test,ypredict)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
